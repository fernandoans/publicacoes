%------------------------------------------------------------------------------------
%	CHAPTER 2
%------------------------------------------------------------------------------------
\chapterimage{headSpark.png}
\chapter{Primeiros Exemplos}

\begin{remark}
"Inovação é a faísca que acende o fogo da mudança e impulsiona o progresso da tecnologia." (Bill Gates) 
\end{remark}

\section{Meu Hello World?}\index{Primeiros Exemplos}
Como todo nosso ambiente montado vamos criar um novo caderno, a primeira ação que devemos realizar é criar uma seção e um contexto para conversação com o Spark. Importamos a biblioteca necessária:
\begin{lstlisting}[]
from pyspark.sql import SparkSession
\end{lstlisting}

Criamos o objeto de seção:
\begin{lstlisting}[]
spark = SparkSession.\
        builder.\
        appName("pyspark-notebook").\
        master("spark://spark-master:7077").\
        config("spark.executor.memory", "512m").\
        getOrCreate()
\end{lstlisting}

Damos um nome para esta seção: \textbf{pyspark-notebook}, indicamos aonde está o Spark Master (conforme o contêiner Docker): spark-master:7077 e quanta memória deve utilizar: 512 Mb. Agora se voltarmos ao gerenciador veremos que os \textbf{Spark Workers} foram alocados para responder a essa seção:
\begin{figure}[H]
	\centering\includegraphics[scale=1.0]{cap02/WorkerAlocado}
	\caption{Seção Running Applications do Spark Master}
\end{figure}

Por fim criamos um contexto de conversação:
\begin{lstlisting}[]
sc = spark.sparkContext
\end{lstlisting}

Com essa célula executando perfeitamente temos o nosso Hello Wolrd pronto! Concordo, não teve a menor graça e parece que ficou faltando algo, bem wm paralelismo é assim mesmo que acontece, mas vamos em frente e verificarmos o que são objetos RDD.

\section{Objeto RDD}\index{Primeiros Exemplos}
Por definição, cada aplicativo \textbf{Spark} consiste em um programa que executa várias operações paralelas em um \textbf{cluster}. A abstração principal que \textbf{Spark} oferece é um conjunto chamado \textbf{RDD} (\textit{Resilient Distributed Dataset}), este é uma coleção de elementos particionados nos nós do \textbf{cluster}. Os RDDs são criados a partir de um arquivo no sistema de arquivos \textbf{Hadoop} (ou qualquer outro sistema de arquivos compatível com Hadoop). As características deste são:
\begin{figure}[H]
	\centering\includegraphics[scale=0.3]{cap02/ArquiteturaRDD}
	\caption{Características do RDD}
\end{figure}

Podem ser mantidos em memória, são trabalhados de forma tardia (Lazy, assim não alocam recursos do programa principal), possuem tolerância a falhas, são objetos imutáveis, são particionados, podem realizar persistência dos dados (guardá-los para recuperação posterior) e possuem baixa granularidade (coarse-grained service) e reduz o número de chamadas necessárias para concluir uma tarefa.

Vamos criar um RDD de forma bem simples através de uma lista:
\begin{lstlisting}[]
rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])
print(type(rdd))
\end{lstlisting}

E como resposta temos: \\
{\ttfamily <class 'pyspark.rdd.RDD'>}

Para vermos os elementos usamos a função collect():
\begin{lstlisting}[]
print(rdd.collect())
\end{lstlisting}

E temos como resposta: \\
{\ttfamily [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}

\section{Uso de Arquivos Texto}\index{Primeiros Exemplos}
Também podemos ler um arquivo texto, para este exemplo criar um arquivo texto na mesma pasta com o nome: teste.txt e seguinte conteúdo:
\begin{lstlisting}[]
Apache Spark é um mecanismo de análise unificado para processamento de dados em grande escala.
Fornece APIs para linguagens de alto nível como Java, Scala, Python e R
Permite um mecanismo otimizado para suporte a gráficos de execução geral.
Possui um excelente conjunto de ferramentas, tais como:
    Spark SQL para SQL e processamento de dados estruturados;
    MLlib para aprendizado de máquina;
    GraphX para processamento de gráfico; e
    Streams estruturados para computação incremental e processamento de fluxo.
\end{lstlisting}

Para ler este arquivo usamos:
\begin{lstlisting}[]
rdd = sc.textFile('teste.txt')
print(type(rdd))
\end{lstlisting}

E observamos que é o mesmo tipo do objeto criado anteriormente, se desejar contar quantos elementos temos:
\begin{lstlisting}[]
print(rdd.count())
\end{lstlisting}

E retorna que temos 8 elementos, como assim? Spark utiliza as linhas como elementos, se quisermos verificar como o RDD foi montado é só usar o método collect()
\begin{lstlisting}[]
print(rdd.collect())
\end{lstlisting}

Obtemos toda sua configuração utilizada com o comando:
\begin{lstlisting}[]
print(sc.getConf().getAll())
\end{lstlisting}

Temos elementos que atrapalham nossos próximos passos, assim para remover vírgulas, dois pontos, ponto final e outros sinais de pontuação do texto em um RDD utilizamos as funções para a manipulação de strings disponíveis no Spark, importamos para isso a biblioteca \textbf{re}:
\begin{lstlisting}[]
import re
\end{lstlisting}

E para substituir esses caracteres por espaços em branco ou removê-los completamente
\begin{lstlisting}[]
rdd_sem_pontuacao = rdd.map(lambda linha: re.sub(r'[^\w\s]', '', linha))
for linha in rdd_sem_pontuacao.collect():
    print(linha)
\end{lstlisting}

Utilizamos a biblioteca do Python para utilizar a função \textbf{sub} e substituir os sinais de pontuação por espaços em branco. A expressão regular mostrada corresponde a qualquer caractere que não seja alfanumérico (w) ou espaço em branco (s).

Dessa forma, a transformação \textbf{map} é aplicada em cada linha do RDD para substituir os sinais de pontuação por espaços em branco. Em seguida, utilizamos o método \textbf{collect()} para coletar e imprimir todas as linhas do RDD sem pontuação.E temos como resultado as linhas livres de pontuação.

Agora podemos contar a quantidade de ocorrências para cada palavra no texto, porém ainda temos um pequeno problema que são as palavras sem significado próprio como "e", "é", "de" e por aí vai. vamos removê-las então:
\begin{lstlisting}[]
# Definir a lista de palavras indesejadas
palavras_indesejadas = ["e", "é", "um", "para", "em", "como", "a", "tais", "como", "de", ""]

# Dividir cada linha em palavras e criar um RDD com todas as palavras
palavras = rdd_sem_pontuacao.flatMap(lambda linha: linha.split(" "))

# Filtrar as palavras indesejadas
palavras_filtradas = palavras.filter(lambda palavra: palavra not in palavras_indesejadas)

# Contar as ocorrências de cada palavra
contagem_palavras = palavras_filtradas.countByValue()

# Imprimir as palavras únicas e suas contagens
print("Palavras únicas encontradas e suas contagens:")
for palavra, contagem in contagem_palavras.items():
    print(palavra, "->", contagem)
\end{lstlisting}

Definimos uma lista de palavras que desejamos remover da contagem. Em seguida, aplicamos o método \textbf{filter()} no \textbf{RDD} que não contém a pontuação usando uma condição lambda e assim filtrar as palavras que não estão presentes na lista de palavras indesejadas.

Contamos as ocorrências das palavras filtradas e imprimimos as palavras únicas encontradas e suas respectivas contagens.

\section{PipelinedRDD e DataFrame}\index{Primeiros Exemplos}
Quando falamos de \textbf{Pipeline} rapidamente associamos ao \textbf{Jenkins} e \textbf{DataFrame} ao \textbf{Pandas}, porém devemos esquecer essa associação aqui, o \textbf{PipelinedRDD} e uma subcoleção de um \textbf{RDD} e o \textbf{DataFrame} é o modo como Spark realiza uma associação de linhas e colunas.

Vamos imaginar que temos uma lista de 1.000 pacientes no qual colhemos os dados de Pressão Arterial, tanto a sistólica (PAS) quanto a diastólica (PAD). Para simularmos esses dados vamos importar a classe Random:
\begin{lstlisting}[]
import random
\end{lstlisting}

E criamos um método de modo a retornar os valores como uma lista:
\begin{lstlisting}[]
def valorPressao(x):
    idt = x
    pas = random.randint(11, 20)
    pad = random.randint(70, 121)
    return (x, pas, pad)
\end{lstlisting}

Definimos para PAS um valor aleatório porém devemos nos manter entre 11 e 19 e para PAD entre 70 e 121 de modo a criarmos dados viáveis. Agora criamos um objeto PipelinedRDD:
\begin{lstlisting}[]
rdd_pipe = sc.parallelize(range(1000)).map(valorPressao)
print(type(rdd_pipe))
\end{lstlisting}

E como resposta temos: \\
{\ttfamily <class 'pyspark.rdd.PipelinedRDD'>}

Podemos visualizar os 10 primeiros registros com:
\begin{lstlisting}[]
rdd_pipe.take(10)
\end{lstlisting}

E transformá-lo em um DataFrame com:
\begin{lstlisting}[]
df = rdd_pipe.toDF(("Id", "PAS", "PAD"))
type(df)
\end{lstlisting}

E como resposta temos: \\
{\ttfamily pyspark.sql.dataframe.DataFrame}

E visualizarmos os registros com:
\begin{lstlisting}[]
df.show()
\end{lstlisting}

Para encerramos o trabalho, não devemos esquecer de sempre fechar a seção, antes mesmo de parar a composição Docker.
\begin{lstlisting}[]
spark.sparkContext.stop()
\end{lstlisting}

E notamos na página de status do \textbf{Spark Master} que os \textbf{Spark Workers} alocados foram para a seção \textbf{Completed Applications}.


\section{Novas Bibliotecas e Arquivo PDF}\index{Primeiros Exemplos}
Neste exemplo permita-me demonstrar a vantagem de estarmos em um contêineres, muitas pessoas pensam que estão presas, não podem atualizar ou fazer modificações no contêiner, isso é totalmente falso. Não estamos presos e podemos sim mudar a nossa vontade o conteúdo de um contêiner como se fosse uma pasta normal no nosso computador.

Vamos aproveitar nosso exemplo de contar palavras e criar algo mais sofisticado, ao invés de ler de um arquivo texto qualquer vamos usar um arquivo PDF. Inicialmente criamos um novo notebook e iniciamos a seção do Spark neste:
\begin{lstlisting}[]
from pyspark.sql import SparkSession

spark = SparkSession.\
        builder.\
        appName("pyspark-notebook").\
        master("spark://spark-master:7077").\
        config("spark.executor.memory", "512m").\
        getOrCreate()
        
sc = spark.sparkContext        
\end{lstlisting}

Com a seção ativa podemos ler o arquivo PDF, só que o Spark não possui esse poder e isso deve ser realizado com a linguagem Python. Existem várias bibliotecas em Python para isso, uma das mais simples e úteis é a \textbf{textract}. Instalamos esta:
\begin{lstlisting}[]
!pip install textract
\end{lstlisting}

Exatamente isso, vamos instalar para dentro do nosso contêiner uma biblioteca que não existe neste, próximo passo é utilizar a biblioteca:
\begin{lstlisting}[]
import textract
\end{lstlisting}

\begin{note}[Onde está o PDF?] 
	Todos os código e arquivos utilizados por este livro estão disponíveis no GitHub no seguinte endereço (\url{https://github.com/fernandoans/publicacoes/tree/master/LivroSpark}), inclusive os fontes deste em LaTex.
\end{note}

Vamos ler o arquivo PDF que nos fornecerá o texto:
\begin{lstlisting}[]
text = textract.process("arquivo.pdf", method='pdfminer')
texto_corr = text.decode('utf-8')
\end{lstlisting}

E para gerarmos o nosso objeto RDD usamos a função \textbf{parallelize()}:
\begin{lstlisting}[]
file_in = sc.parallelize([texto_corr])
\end{lstlisting}

Para sabermos a quantidade de caracteres que temos no arquivo, podemos utilizar os métodos \textbf{map()} e \textbf{reduce()}, da seguinte forma:
\begin{lstlisting}[]
# Adicionar o operador add
from operator import add

# Contar quantos caracteres
chars = file_in.map(lambda s: len(s)).reduce(add)
print('Número de caracteres no arquivo: %s' % chars)
\end{lstlisting}

E temos como resposta que são 17.019 caracteres encontrados em nosso texto. Essa parte agora é bem parecida com o que vimos, mas vou pecar pelo excesso de informação neste:
\begin{lstlisting}[]
# importar o módulo regex
import re

# Definir a lista de palavras indesejadas
palavras_indesejadas = ["404", "agosto", "julho", "aulapython", "minicursos", "count", "erad", "2021", "para", "como", "como", "possui", "uma", "que", "com", "são", "quando", "linha", "pode", "ser", "ERAD-CO"]

# Dividir cada linha em palavras e criar um RDD com todas as palavras
palavras = file_in.flatMap(lambda linha: linha.split(" "))

# Colocar todas as palavras em letras minúsculas
palavras = palavras.flatMap(lambda line: re.split('\W+', line.lower().strip()))

# Filtrar as palavras indesejadas
palavras = palavras.filter(lambda palavra: palavra not in palavras_indesejadas)

# Colocar todas as palavras em letras minúsculas
palavras = palavras.flatMap(lambda line: re.split('\W+', line.lower().strip()))

# Filtrar apenas as palavras que não possuam caracteres
palavras = palavras.filter(lambda x: len(x) > 2)

# Fazer com que cada palavra tenha o valor 1
palavras = palavras.map(lambda w: (w,1))

# Reduzir para o total
palavras = palavras.reduceByKey(add)
\end{lstlisting}

Tudo o que fizemos foi arrumar a informação eliminando as palavras que não interessam, retirar espaços em branco, trocar todas as palavras para letras minúsculas (poderíamos também usar o método upper() para todas maiúsculas), remover as palavras menores que 3 caracteres, fazer com que cada palavra possua o valor 1 e reduzir para achar o total de palavras. E para acharmos nossa lista Top 10 de palavras mais comuns:
\begin{lstlisting}[]
# Criar uma tupla (count, word) e ordenar de forma ascendente
tpalavras = palavras.map(lambda x:(x[1],x[0])).sortByKey(False)

# Pegar o Top 10
tpalavras.take(10)
\end{lstlisting}

Criamos uma tupla de palavras e ordenamos de forma ascendente e mostramos as 10 primeiras palavras. Se observarmos o tipo desse objeto veremos que temos como resultado do método \textbf{take()} uma lista, em cada elemento temos uma tupla contendo a quantidade e sua palavra respectiva. 

Creio que falta algo quando mostramos a informação dessa maneira. Para isso precisamos instalar a biblioteca MatPlotLib:
\begin{lstlisting}[]
!pip3 install matplotlib
\end{lstlisting}

Agora ficou fácil, basta importarmos a biblioteca e marcar que o gráfico deve aparecer como uma célula corrente:
\begin{lstlisting}[]
import matplotlib.pyplot as plt
%matplotlib inline
\end{lstlisting}

E plotar o gráfico:
\begin{lstlisting}[]
# Obter os dados
dados = tpalavras.take(10)

# Inverter a ordem dos dados
dados = list(reversed(dados))

# Extrair os valores e rótulos da lista
values = [item[0] for item in dados]
labels = [item[1] for item in dados]

# Criar um gráfico de barras horizontais
plt.barh(range(len(dados)), values, align='center')
plt.yticks(range(len(dados)), labels)

# Definir os rótulos dos eixos
plt.xlabel('Contagem')
plt.ylabel('Palavra')

# Exibir o gráfico
plt.show()
\end{lstlisting}

Obtemos a lista das Top 10 palavras, invertemos a ordem dos dados, isso é importante pois um gráfico assim deve aparecer na ordem da palavra que aparece mais para a menor, obtemos os \textit{values} e \textit{labels} para montamos o gráfico final que tem como resultado:
\begin{figure}[H]
	\centering\includegraphics[scale=0.45]{cap02/GraficoPalavras}
	\caption{Top 10 das Palavras do Texto}
\end{figure}

É interessante saber que nem precisamos abrir o PDF e ler todo o texto para sabermos qual o assunto tratado neste, indo um pouco além, imaginemos isso para classificar a informação de todos os PDF de uma coleção com os assuntos tratados por cada um deles. Por fim não esquecer de encerrar a seção:
\begin{lstlisting}[]
spark.sparkContext.stop()
\end{lstlisting}

\clearpage
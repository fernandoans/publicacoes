%------------------------------------------------------------------------------------
%	CHAPTER 2
%------------------------------------------------------------------------------------
\chapterimage{headSpark.png}
\chapter{Primeiros Exemplos}

\begin{remark}
"Inovação é a faísca que acende o fogo da mudança e impulsiona o progresso da tecnologia." (Bill Gates) 
\end{remark}

\section{Meu Hello World?}\index{Primeiros Exemplos}
Como todo nosso ambiente montado vamos criar um novo caderno, a primeira ação que devemos realizar é criar uma seção e um contexto para conversação com o Spark. Importamos a biblioteca necessária:
\begin{lstlisting}[]
from pyspark.sql import SparkSession
\end{lstlisting}

Criamos o objeto de seção:
\begin{lstlisting}[]
spark = SparkSession.\
        builder.\
        appName("pyspark-notebook").\
        master("spark://spark-master:7077").\
        config("spark.executor.memory", "512m").\
        getOrCreate()
\end{lstlisting}

Damos um nome para esta seção: \textbf{pyspark-notebook}, indicamos aonde está o Spark Master (conforme o contêiner Docker): spark-master:7077 e quanta memória deve utilizar: 512 Mb. Agora se voltarmos ao gerenciador veremos que os \textbf{Spark Workers} foram alocados para responder a essa seção:
\begin{figure}[H]
	\centering\includegraphics[scale=1.0]{cap02/WorkerAlocado}
	\caption{Seção Running Applications do Spark Master}
\end{figure}

Voltemos para o Notebook e em uma nova célula digite:
\begin{lstlisting}[]
spark
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
SparkSession - in-memory
SparkContext
Spark UI
Version            v3.4.1
Master             spark://spark-master:7077
AppName            pyspark-notebook
\end{verbatim}

Por fim criamos um contexto de conversação:
\begin{lstlisting}[]
sc = spark.sparkContext
\end{lstlisting}

Com essa célula executando perfeitamente temos o nosso Hello Wolrd pronto! Concordo, não teve a menor graça e parece que ficou faltando algo, bem wm paralelismo é assim mesmo que acontece, mas vamos em frente e verificarmos o que são objetos RDD.

\section{Objeto RDD}\index{Primeiros Exemplos}
Por definição, cada aplicativo \textbf{Spark} consiste em um programa que executa várias operações paralelas em um \textbf{cluster}. A abstração principal que \textbf{Spark} oferece é um conjunto chamado \textbf{RDD} (\textit{Resilient Distributed Dataset}), este é uma coleção de elementos particionados nos nós do \textbf{cluster}. Os RDDs são criados a partir de um arquivo no sistema de arquivos \textbf{Hadoop} (ou qualquer outro sistema de arquivos compatível com Hadoop). As características deste são:
\begin{figure}[H]
	\centering\includegraphics[scale=0.3]{cap02/ArquiteturaRDD}
	\caption{Características do RDD}
\end{figure}

Podem ser mantidos em memória, são trabalhados de forma tardia (Lazy, assim não alocam recursos do programa principal), possuem tolerância a falhas, são objetos imutáveis, são particionados, podem realizar persistência dos dados (guardá-los para recuperação posterior) e possuem baixa granularidade (coarse-grained service) e reduz o número de chamadas necessárias para concluir uma tarefa.

Vamos criar um RDD de forma bem simples através de uma lista:
\begin{lstlisting}[]
rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])
print(type(rdd))
\end{lstlisting}

E como resposta temos: \\
{\ttfamily <class 'pyspark.rdd.RDD'>}

Para vermos os elementos usamos a função collect():
\begin{lstlisting}[]
print(rdd.collect())
\end{lstlisting}

E temos como resposta: \\
{\ttfamily [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}

\section{Uso de Arquivos Texto}\index{Primeiros Exemplos}
Também podemos ler um arquivo texto, para este exemplo criar um arquivo texto na mesma pasta com o nome: teste.txt e seguinte conteúdo:
\begin{lstlisting}[]
Apache Spark é um mecanismo de análise unificado para processamento de dados em grande escala.
Fornece APIs para linguagens de alto nível como Java, Scala, Python e R
Permite um mecanismo otimizado para suporte a gráficos de execução geral.
Possui um excelente conjunto de ferramentas, tais como:
    Spark SQL para SQL e processamento de dados estruturados;
    MLlib para aprendizado de máquina;
    GraphX para processamento de gráfico; e
    Streams estruturados para computação incremental e processamento de fluxo.
\end{lstlisting}

Para ler este arquivo usamos:
\begin{lstlisting}[]
rdd = sc.textFile('teste.txt')
print(type(rdd))
\end{lstlisting}

E observamos que é o mesmo tipo do objeto criado anteriormente, se desejar contar quantos elementos temos:
\begin{lstlisting}[]
print(rdd.count())
\end{lstlisting}

E retorna que temos 8 elementos, como assim? Spark utiliza as linhas como elementos, se quisermos verificar como o RDD foi montado é só usar o método collect()
\begin{lstlisting}[]
print(rdd.collect())
\end{lstlisting}

Obtemos toda sua configuração utilizada com o comando:
\begin{lstlisting}[]
print(sc.getConf().getAll())
\end{lstlisting}

Temos elementos que atrapalham nossos próximos passos, assim para remover vírgulas, dois pontos, ponto final e outros sinais de pontuação do texto em um RDD utilizamos as funções para a manipulação de strings disponíveis no Spark, importamos para isso a biblioteca \textbf{re}:
\begin{lstlisting}[]
import re
\end{lstlisting}

E para substituir esses caracteres por espaços em branco ou removê-los completamente
\begin{lstlisting}[]
rdd_sem_pontuacao = rdd.map(lambda linha: re.sub(r'[^\w\s]', '', linha))
for linha in rdd_sem_pontuacao.collect():
    print(linha)
\end{lstlisting}

Utilizamos a biblioteca do Python para utilizar a função \textbf{sub} e substituir os sinais de pontuação por espaços em branco. A expressão regular mostrada corresponde a qualquer caractere que não seja alfanumérico (w) ou espaço em branco (s).

Dessa forma, a transformação \textbf{map} é aplicada em cada linha do RDD para substituir os sinais de pontuação por espaços em branco. Em seguida, utilizamos o método \textbf{collect()} para coletar e imprimir todas as linhas do RDD sem pontuação.E temos como resultado as linhas livres de pontuação.

Agora podemos contar a quantidade de ocorrências para cada palavra no texto, porém ainda temos um pequeno problema que são as palavras sem significado próprio como "e", "é", "de" e por aí vai. vamos removê-las então:
\begin{lstlisting}[]
# Definir a lista de palavras indesejadas
palavras_indesejadas = ["e", "é", "um", "para", "em", "como", "a", "tais", "como", "de", ""]

# Dividir cada linha em palavras e criar um RDD com todas as palavras
palavras = rdd_sem_pontuacao.flatMap(lambda linha: linha.split(" "))

# Filtrar as palavras indesejadas
palavras_filtradas = palavras.filter(lambda palavra: palavra not in palavras_indesejadas)

# Contar as ocorrências de cada palavra
contagem_palavras = palavras_filtradas.countByValue()

# Imprimir as palavras únicas e suas contagens
print("Palavras únicas encontradas e suas contagens:")
for palavra, contagem in contagem_palavras.items():
    print(palavra, "->", contagem)
\end{lstlisting}

Definimos uma lista de palavras que desejamos remover da contagem. Em seguida, aplicamos o método \textbf{filter()} no \textbf{RDD} que não contém a pontuação usando uma condição lambda e assim filtrar as palavras que não estão presentes na lista de palavras indesejadas.

Contamos as ocorrências das palavras filtradas e imprimimos as palavras únicas encontradas e suas respectivas contagens.

\section{PipelinedRDD e Dataframe}\index{Primeiros Exemplos}
Quando falamos de \textbf{Pipeline} rapidamente associamos ao \textbf{Jenkins} e \textbf{Dataframe} ao \textbf{Pandas}, porém devemos esquecer essa associação aqui, o \textbf{PipelinedRDD} e uma subcoleção de um \textbf{RDD} e o \textbf{Dataframe} é o modo como Spark realiza uma associação de linhas e colunas.

Vamos imaginar que temos uma lista de 1.000 pacientes no qual colhemos os dados de Pressão Arterial, tanto a sistólica (PAS) quanto a diastólica (PAD). Para simularmos esses dados vamos importar a classe Random:
\begin{lstlisting}[]
import random
\end{lstlisting}

E criamos um método de modo a retornar os valores como uma lista:
\begin{lstlisting}[]
def valorPressao(x):
    idt = x
    pas = random.randint(11, 20)
    pad = random.randint(70, 121)
    return (x, pas, pad)
\end{lstlisting}

Definimos para PAS um valor aleatório porém devemos nos manter entre 11 e 19 e para PAD entre 70 e 121 de modo a criarmos dados viáveis. Agora criamos um objeto PipelinedRDD:
\begin{lstlisting}[]
rdd_pipe = sc.parallelize(range(1000)).map(valorPressao)
print(type(rdd_pipe))
\end{lstlisting}

E como resposta temos: \\
{\ttfamily <class 'pyspark.rdd.PipelinedRDD'>}

Podemos visualizar os 10 primeiros registros com:
\begin{lstlisting}[]
rdd_pipe.take(10)
\end{lstlisting}

E transformá-lo em um Dataframe com:
\begin{lstlisting}[]
df = rdd_pipe.toDF(("Id", "PAS", "PAD"))
type(df)
\end{lstlisting}

E como resposta temos: \\
{\ttfamily pyspark.sql.Dataframe.Dataframe}

E visualizarmos os registros com:
\begin{lstlisting}[]
df.show()
\end{lstlisting}

Para encerramos o trabalho, não devemos esquecer de sempre fechar a seção, antes mesmo de parar a composição Docker.
\begin{lstlisting}[]
spark.sparkContext.stop()
\end{lstlisting}

E notamos na página de status do \textbf{Spark Master} que os \textbf{Spark Workers} alocados foram para a seção \textbf{Completed Applications}.

\section{Novas Bibliotecas e Arquivo PDF}\index{Primeiros Exemplos}
Neste exemplo permita-me demonstrar a vantagem de estarmos em um contêineres, muitas pessoas pensam que estão presas, não podem atualizar ou fazer modificações no contêiner, isso é totalmente falso. Não estamos presos e podemos sim mudar a nossa vontade o conteúdo de um contêiner como se fosse uma pasta normal no nosso computador.

Vamos aproveitar nosso exemplo de contar palavras e criar algo mais sofisticado, ao invés de ler de um arquivo texto qualquer vamos usar um arquivo PDF. Inicialmente criamos um novo notebook e iniciamos a seção do Spark neste:
\begin{lstlisting}[]
from pyspark.sql import SparkSession

spark = SparkSession.\
        builder.\
        appName("pyspark-notebook").\
        master("spark://spark-master:7077").\
        config("spark.executor.memory", "512m").\
        getOrCreate()
        
sc = spark.sparkContext        
\end{lstlisting}

Com a seção ativa podemos ler o arquivo PDF, só que o Spark não possui esse poder e isso deve ser realizado com a linguagem Python. Existem várias bibliotecas em Python para isso, uma das mais simples e úteis é a \textbf{textract}. Instalamos esta:
\begin{lstlisting}[]
!pip install textract
\end{lstlisting}

Exatamente isso, vamos instalar para dentro do nosso contêiner uma biblioteca que não existe neste, próximo passo é utilizar a biblioteca:
\begin{lstlisting}[]
import textract
\end{lstlisting}

\begin{note}[Onde está o PDF?] 
	Todos os código e arquivos utilizados por este livro estão disponíveis no GitHub no seguinte endereço (\url{https://github.com/fernandoans/publicacoes/tree/master/LivroSpark}), inclusive os fontes deste em LaTex.
\end{note}

Vamos ler o arquivo PDF que nos fornecerá o texto:
\begin{lstlisting}[]
text = textract.process("arquivo.pdf", method='pdfminer')
texto_corr = text.decode('utf-8')
\end{lstlisting}

E para gerarmos o nosso objeto RDD usamos a função \textbf{parallelize()}:
\begin{lstlisting}[]
file_in = sc.parallelize([texto_corr])
\end{lstlisting}

RDD representa uma coleção de elementos particionados que podem ser operados em paralelo. É o mecanismo primário ds abstração de dados no Spark. É definido como uma classe abstrata na biblioteca Spark. Para sabermos a quantidade de caracteres, podemos utilizar os métodos \textbf{map()} e \textbf{reduce()}, da seguinte forma:
\begin{lstlisting}[]
# Adicionar o operador add
from operator import add

# Contar quantos caracteres
chars = file_in.map(lambda s: len(s)).reduce(add)
print('Número de caracteres no arquivo: %s' % chars)
\end{lstlisting}

E temos como resposta que são 17.019 caracteres encontrados em nosso texto. Essa parte agora é bem parecida com o que vimos, mas vou pecar pelo excesso de informação neste:
\begin{lstlisting}[]
# importar o módulo regex
import re

# Definir a lista de palavras indesejadas
palavras_indesejadas = ["404", "agosto", "julho", "aulapython", "minicursos", "count", "erad", "2021", "para", "como", "como", "possui", "uma", "que", "com", "são", "quando", "linha", "pode", "ser", "ERAD-CO"]

# Dividir cada linha em palavras e criar um RDD com todas as palavras
palavras = file_in.flatMap(lambda linha: linha.split(" "))

# Colocar todas as palavras em letras minúsculas
palavras = palavras.flatMap(lambda line: re.split('\W+', line.lower().strip()))

# Filtrar as palavras indesejadas
palavras = palavras.filter(lambda palavra: palavra not in palavras_indesejadas)

# Colocar todas as palavras em letras minúsculas
palavras = palavras.flatMap(lambda line: re.split('\W+', line.lower().strip()))

# Filtrar apenas as palavras que não possuam caracteres
palavras = palavras.filter(lambda x: len(x) > 2)

# Fazer com que cada palavra tenha o valor 1
palavras = palavras.map(lambda w: (w,1))

# Reduzir para o total
palavras = palavras.reduceByKey(add)
\end{lstlisting}

Tudo o que fizemos foi arrumar a informação eliminando as palavras que não interessam, retirar espaços em branco, trocar todas as palavras para letras minúsculas (poderíamos também usar o método upper() para todas maiúsculas), remover as palavras menores que 3 caracteres, fazer com que cada palavra possua o valor 1 e reduzir para achar o total de palavras. E para acharmos nossa lista Top 10 de palavras mais comuns:
\begin{lstlisting}[]
# Criar uma tupla (count, word) e ordenar de forma ascendente
tpalavras = palavras.map(lambda x:(x[1],x[0])).sortByKey(False)

# Pegar o Top 10
tpalavras.take(10)
\end{lstlisting}

Também podemos utilizar o método \textit{top()}:
\begin{lstlisting}[]
# Criar uma tupla (count, word)
tpalavras = palavras.map(lambda x:(x[1],x[0]))

# Pegar o Top 10
tpalavras.top(10)
\end{lstlisting}

A diferença é que o método \textit{take()} obtém as 10 primeiras palavras da lista, enquanto que o segundo obtém as 10 primeiras com o maior valor da lista (utiliza o key da tupla para definir qual o maior), deste modo não precisamos do método \textit{sortByKey()}.

Criamos uma tupla de palavras e ordenamos de forma ascendente e mostramos as 10 primeiras palavras. Se observarmos o tipo desse objeto veremos que temos como resultado do método \textbf{take()} uma lista, em cada elemento temos uma tupla contendo a quantidade e sua palavra respectiva. 

Creio que falta algo quando mostramos a informação dessa maneira. Para isso precisamos instalar a biblioteca MatPlotLib:
\begin{lstlisting}[]
!pip3 install matplotlib
\end{lstlisting}

Agora ficou fácil, basta importarmos a biblioteca e marcar que o gráfico deve aparecer como uma célula corrente:
\begin{lstlisting}[]
import matplotlib.pyplot as plt
%matplotlib inline
\end{lstlisting}

E plotar o gráfico:
\begin{lstlisting}[]
# Obter os dados
dados = tpalavras.top(10)

# Inverter a ordem dos dados
dados = list(reversed(dados))

# Extrair os valores e rótulos da lista
values = [item[0] for item in dados]
labels = [item[1] for item in dados]

# Criar um gráfico de barras horizontais
plt.barh(range(len(dados)), values, align='center')
plt.yticks(range(len(dados)), labels)

# Definir os rótulos dos eixos
plt.xlabel('Contagem')
plt.ylabel('Palavra')

# Exibir o gráfico
plt.show()
\end{lstlisting}

Obtemos a lista das Top 10 palavras, invertemos a ordem dos dados, isso é importante pois um gráfico assim deve aparecer na ordem da palavra que aparece mais para a menor, obtemos os \textit{values} e \textit{labels} para montamos o gráfico final que tem como resultado:
\begin{figure}[H]
	\centering\includegraphics[scale=0.45]{cap02/GraficoPalavras}
	\caption{Top 10 das Palavras do Texto}
\end{figure}

É interessante saber que nem precisamos abrir o PDF e ler todo o texto para sabermos qual o assunto tratado neste, indo um pouco além, imaginemos isso para classificar a informação de todos os PDF de uma coleção com os assuntos tratados por cada um deles. Por fim não esquecer de encerrar a seção:
\begin{lstlisting}[]
spark.sparkContext.stop()
\end{lstlisting}

\section{Arquivos CSV e mais sobre Dataframes}\index{Primeiros Exemplos}
Neste exemplo, aprenderemos mais sobre \textit{Dataframes} e suas manipulações. Provavelmente poderíamos realizar associações com os \textit{Dataframes} da biblioteca Pandas, mas não se confunda, \textit{Dataframes} em Spark são outros elementos completamente diferentes, apesar de seguirem o mesmo conceito de linhas e colunas.

Neste exemplo, veremos como criar e manipular \textit{Dataframes} usando o \textbf{PySpark}. Além disso, exploraremos como modificar suas colunas, obter seus dados e também aprenderemos a adicionar e remover colunas. 

Começemos iniciando a seção e criamos o objeto de seção:
\begin{lstlisting}[]
from pyspark.sql import SparkSession

spark = SparkSession.\
        builder.\
        appName("pyspark-notebook").\
        master("spark://spark-master:7077").\
        config("spark.executor.memory", "512m").\
        getOrCreate()
\end{lstlisting}

Como vamos utilizar exclusivamente um \textit{Dataframe} não precisamos usar o contexto e usaremos diretamente o objeto de seção. Nosso próximo passo é criar o \textit{Dataframe}:
\begin{lstlisting}[]
df_pyspark = spark.read.csv('dados.csv')
print(type(df_pyspark))
\end{lstlisting}

Como resposta temos: \\
{\ttfamily pyspark.sql.dataframe.DataFrame}

Verificamos que foi criado corretamente o \textit{Dataframe}, o arquivo "dados.csv" possui a seguinte codificação:
\begin{lstlisting}[]
Nome,Idade,Experiencia
Maria,50,360
Fernando,56,444
Salvia,28,264
Joana,47,240
Souza,73,144
\end{lstlisting}

Vamos agora visualizar esse \textit{DataFrame}:
\begin{lstlisting}[]
df_pyspark.show()
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
+--------+-----+-----------+
|     _c0|  _c1|        _c2|
+--------+-----+-----------+
|    Nome|Idade|Experiencia|
|   Maria|   50|        360|
|Fernando|   56|        444|
|  Salvia|   28|        264|
|   Joana|   47|        240|
|   Souza|   73|        144|
+--------+-----+-----------+
\end{verbatim}

Conseguimos criar porém está bem estranho, pois cabeçalho agora faz parte da linha, para corrigirmos isso utilizamos a seguinte sintaxe:
\begin{lstlisting}[]
df_pyspark = spark.read.option('header', 'true').csv('dados.csv')
df_pyspark.show()
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
+--------+-----+-----------+
|    Nome|Idade|Experiencia|
+--------+-----+-----------+
|Fernando|   56|        444|
|   Joana|   47|         20|
|   Maria|   50|         30|
|   Souza|   73|         12|
|  Salvia|   28|         22|
+--------+-----+-----------+
\end{verbatim}

Podemos verificar o esquema criado com o seguinte comando:
\begin{lstlisting}[]
df_pyspark.printSchema()
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
root
 |-- Nome: string (nullable = true)
 |-- Idade: string (nullable = true)
 |-- Experiencia: string (nullable = true)
\end{verbatim}

Observamos que temos dois tipos errados, "idade" e "Experiencia", para corrigi-los podemos usar a seguinte sintaxe:
\begin{lstlisting}[]
df_pyspark = spark.read.option('header', 'true').csv('dados.csv', inferSchema=True)
df_pyspark.printSchema()
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
root
 |-- Nome: string (nullable = true)
 |-- Idade: integer (nullable = true)
 |-- Experiencia: integer (nullable = true)
\end{verbatim}

Corrigindo nossos tipos, podemos também carregar o Dataframe com a seguinte sintaxe, mais usual:
\begin{lstlisting}[]
df_pyspark = spark.read.csv('dados.csv', header=True, inferSchema=True)
\end{lstlisting}

Outro modo de verificar os tipos de dados é através da sintaxe:
\begin{lstlisting}[]
df_pyspark.dtypes
\end{lstlisting}

A diferença é que o resultado será em formato de uma lista de tuplas: \vspace{-1.5em}
\begin{verbatim}
[('Nome', 'string'),
 ('Idade', 'int'),
 ('Experiencia', 'int')]
\end{verbatim}

Um comando bem conhecido do Pandas é para mostrar dados estatísticos conseguido pelo método \textbf{describe()} porém no PySpark é feito uma pequena mudança:
\begin{lstlisting}[]
df_pyspark.describe().show()
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
+-------+--------+-----------------+------------------+
|summary|    Nome|            Idade|       Experiencia|
+-------+--------+-----------------+------------------+
|  count|       5|                5|                 5|
|   mean|    null|             50.8|             290.4|
| stddev|    null|16.23884232326923|115.22499728791492|
|    min|Fernando|               28|               144|
|    max|   Souza|               73|               444|
+-------+--------+-----------------+------------------+
\end{verbatim}

No resultado, podemos observar que as colunas não numéricas também são mostradas, porém com alguns dos valores matemáticos nulos. Porém comparando com o comando do Pandas isso é pobre de informações. Resolveremos este problema no próximo capítulo.

\subsection{Trabalhar com Colunas}\index{Primeiros Exemplos}
Para abordarmos as colunas, podemos pegar o título dessas como uma lista:
\begin{lstlisting}[]
df_pyspark.columns
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
['Nome', 'Idade', 'Experiencia']
\end{verbatim}

Outro comando parecido com o Pandas é:
\begin{lstlisting}[]
df_pyspark.head(3)
\end{lstlisting}

Que retorna os três primeiros elementos, porém com um resultando bem diferente: \vspace{-1.5em}
\begin{verbatim}
[Row(Nome='Maria', Idade=50, Experiencia=360),
 Row(Nome='Fernando', Idade=56, Experiencia=444),
 Row(Nome='Salvia', Idade=28, Experiencia=264)]
\end{verbatim}

O método select(), nos permite selecionar uma determinada coluna:
\begin{lstlisting}[]
df_pyspark.select('Idade').show()
\end{lstlisting}

Ou selecionar várias colunas:
\begin{lstlisting}[]
df_pyspark.select(['Nome', 'Idade']).show()
\end{lstlisting}

Ao importarmos a biblioteca de funções (\textit{functions}) podemos realizar um select() mais elaborado, como:
\begin{lstlisting}[]
from pyspark.sql import functions as F
df_pyspark.select(F.col("Idade").cast('float').alias("Idade em Float")).show()
\end{lstlisting}

Que nos retorna o resultado em ponto flutuante. Falando nisso, podemos criar uma nova coluna com a Experiência em ano ao invés de meses como está descrita:
\begin{lstlisting}[]
df_pyspark = df_pyspark.withColumn('Experiencia_Anos', df_pyspark['Experiencia']/12)
df_pyspark.show()
\end{lstlisting}

Porém observamos que o resultado: \vspace{-1.5em}
\begin{verbatim}
+--------+-----+-----------+----------------+
|    Nome|Idade|Experiencia|Experiencia_Anos|
+--------+-----+-----------+----------------+
|   Maria|   50|        360|            30.0|
|Fernando|   56|        444|            37.0|
|  Salvia|   28|        264|            22.0|
|   Joana|   47|        240|            20.0|
|   Souza|   73|        144|            12.0|
+--------+-----+-----------+----------------+
\end{verbatim}

É do tipo ponto flutuante, podemos mudar isto com:
\begin{lstlisting}[]
df_pyspark = df_pyspark.withColumn("Experiencia_Anos", df_pyspark.Experiencia_Anos.cast('int'))
df_pyspark.show()
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
+--------+-----+-----------+----------------+
|    Nome|Idade|Experiencia|Experiencia_Anos|
+--------+-----+-----------+----------------+
|   Maria|   50|        360|              30|
|Fernando|   56|        444|              37|
|  Salvia|   28|        264|              22|
|   Joana|   47|        240|              20|
|   Souza|   73|        144|              12|
+--------+-----+-----------+----------------+
\end{verbatim}

Como não precisamos mais da experiência em meses, podemos eliminar essa coluna:
\begin{lstlisting}[]
df_pyspark = df_pyspark.drop('Experiencia')
df_pyspark.show()
\end{lstlisting}

E renomear a coluna Idade para refletir o tempo correto e não causar mais futuras confusões temporais:
\begin{lstlisting}[]
df_pyspark = df_pyspark.withColumnRenamed('Idade', 'Idade_Anos')
df_pyspark.show()
\end{lstlisting}

E temos como resultado final: \vspace{-1.5em}
\begin{verbatim}
+--------+---------------+---------------------+
|    Nome|Idade (em Anos)|Experiencia (em Anos)|
+--------+---------------+---------------------+
|   Maria|             50|                   30|
|Fernando|             56|                   37|
|  Salvia|             28|                   22|
|   Joana|             47|                   20|
|   Souza|             73|                   12|
+--------+---------------+---------------------+
\end{verbatim}

Podemos gerar novas colunas através da concatenação (junção) de duas outras, por exemplo, o login das pessoas é definico como seu nome e idade:
\begin{lstlisting}[]
df_pyspark = df_pyspark.withColumn('Login',F.concat('Nome','Idade_Anos'))
df_pyspark.show()
\end{lstlisting}

E temos como resultado final: \vspace{-1.5em}
\begin{verbatim}
+--------+----------+----------------+----------+
|    Nome|Idade_Anos|Experiencia_Anos|     Login|
+--------+----------+----------------+----------+
|   Maria|        50|              30|   Maria50|
|Fernando|        56|              37|Fernando56|
|  Salvia|        28|              22|  Salvia28|
|   Joana|        47|              20|   Joana47|
|   Souza|        73|              12|   Souza73|
+--------+----------+----------------+----------+
\end{verbatim}

\subsection{Criação Dinâmica}\index{Primeiros Exemplos}
Também podemos criar um \textit{Dataframe} a partir de dados dinâmicos (gerados pelo nosso programa), para isso apenas precisamos de uma lista, sendo que cada elemento é uma tupla:
\begin{lstlisting}[]
simpleData = [
    ("Fernando",34,"2006-01-03","True","M",3000.60),
    ("Soraia",33,"1980-01-10","True","F",3300.80),
    ("João",37,"1992-06-01","False","M",5000.50)
  ]

# Nome das colunas
columns = ["Nome","Idade","Inicio","Graduacao","Genero","Salario"]

# Criar o DataFrame
df = spark.createDataFrame(data = simpleData, schema = columns)
df.show(truncate=False)
\end{lstlisting}

E temos como resultado final: \vspace{-1.5em}
\begin{verbatim}
+--------+-----+----------+---------+------+-------+
|Nome    |Idade|Inicio    |Graduacao|Genero|Salario|
+--------+-----+----------+---------+------+-------+
|Fernando|34   |2006-01-03|True     |M     |3000.6 |
|Soraia  |33   |1980-01-10|True     |F     |3300.8 |
|João    |37   |1992-06-01|False    |M     |5000.5 |
+--------+-----+----------+---------+------+-------+
\end{verbatim}

Porém ao verificarmos o esquema do nosso Dataframe:
\begin{lstlisting}[]
df.printSchema()
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
root
 |-- Nome: string (nullable = true)
 |-- Idade: long (nullable = true)
 |-- Inicio: string (nullable = true)
 |-- Graduacao: string (nullable = true)
 |-- Genero: string (nullable = true)
 |-- Salario: double (nullable = true)
\end{verbatim}

Reparamos que as colunas: Idade (valor long), Inicio (valor string) e Graduacao (valor string) estão com os tipos errados, devemos então corrigi-los:
\begin{lstlisting}[]
df = df.withColumn("Idade", df.Idade.cast('int'))
df = df.withColumn("Inicio", df.Inicio.cast('date'))
df = df.withColumn("Graduacao", df.Graduacao.cast('boolean'))
\end{lstlisting}

E ao verificarmos novamente:
\begin{lstlisting}[]
df.printSchema()
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
root
 |-- Nome: string (nullable = true)
 |-- Idade: integer (nullable = true)
 |-- Inicio: date (nullable = true)
 |-- Graduacao: boolean (nullable = true)
 |-- Genero: string (nullable = true)
 |-- Salario: double (nullable = true)
\end{verbatim}

E observamos um detalhe curioso, ao mostrarmos novamente o \textit{Dataframe} repare na coluna Graduacao do tipo lógico:
\begin{lstlisting}[]
df.show()
\end{lstlisting}

E temos o seguinte resultado: \vspace{-1.5em}
\begin{verbatim}
+--------+-----+----------+---------+------+-------+
|    Nome|Idade|    Inicio|Graduacao|Genero|Salario|
+--------+-----+----------+---------+------+-------+
|Fernando|   34|2006-01-03|     true|     M| 3000.6|
|  Soraia|   33|1980-01-10|     true|     F| 3300.8|
|    João|   37|1992-06-01|    false|     M| 5000.5|
+--------+-----+----------+---------+------+-------+
\end{verbatim}

Em Python os lógicos são em letras maiúsculas (\textit{True} e \textit{False}), porém em Spark eles são escritos em minúsculas (\textit{true} e \textit{false}) seguindo o padrão correto das linguagens derivas do C. Por fim não esquecer de encerrar a seção:
\begin{lstlisting}[]
spark.sparkContext.stop()
\end{lstlisting}

No próximo capítulo trataremos mais sobre meios estatísticos com o Spark.

\clearpage
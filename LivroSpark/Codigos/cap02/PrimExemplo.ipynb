{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/15 18:38:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de RDD Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "print(type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de RDD com Arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile('teste.txt')\n",
    "print(type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apache Spark é um mecanismo de análise unificado para processamento de dados em grande escala. ', 'Fornece APIs para linguagens de alto nível como Java, Scala, Python e R', 'Permite um mecanismo otimizado para suporte a gráficos de execução geral. ', 'Possui um excelente conjunto de ferramentas, tais como: ', '    Spark SQL para SQL e processamento de dados estruturados;', '    MLlib para aprendizado de máquina;', '    GraphX para processamento de gráfico; e', '    Streams estruturados para computação incremental e processamento de fluxo.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.startTime', '1689446298048'), ('spark.executor.memory', '512m'), ('spark.app.submitTime', '1689446297917'), ('spark.executor.id', 'driver'), ('spark.app.name', 'pyspark-notebook'), ('spark.app.id', 'app-20230715183819-0002'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.port', '42675'), ('spark.rdd.compress', 'True'), ('spark.master', 'spark://spark-master:7077'), ('spark.serializer.objectStreamReset', '100'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.driver.host', 'a7fc22c74fb9'), ('spark.ui.showConsoleProgress', 'true'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]\n"
     ]
    }
   ],
   "source": [
    "print(sc.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark é um mecanismo de análise unificado para processamento de dados em grande escala \n",
      "Fornece APIs para linguagens de alto nível como Java Scala Python e R\n",
      "Permite um mecanismo otimizado para suporte a gráficos de execução geral \n",
      "Possui um excelente conjunto de ferramentas tais como \n",
      "    Spark SQL para SQL e processamento de dados estruturados\n",
      "    MLlib para aprendizado de máquina\n",
      "    GraphX para processamento de gráfico e\n",
      "    Streams estruturados para computação incremental e processamento de fluxo\n"
     ]
    }
   ],
   "source": [
    "rdd_sem_pontuacao = rdd.map(lambda linha: re.sub(r'[^\\w\\s]', '', linha))\n",
    "for linha in rdd_sem_pontuacao.collect():\n",
    "    print(linha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras únicas encontradas e suas contagens:\n",
      "Apache -> 1\n",
      "Spark -> 2\n",
      "mecanismo -> 2\n",
      "análise -> 1\n",
      "unificado -> 1\n",
      "processamento -> 4\n",
      "dados -> 2\n",
      "grande -> 1\n",
      "escala -> 1\n",
      "Fornece -> 1\n",
      "APIs -> 1\n",
      "linguagens -> 1\n",
      "alto -> 1\n",
      "nível -> 1\n",
      "Java -> 1\n",
      "Scala -> 1\n",
      "Python -> 1\n",
      "R -> 1\n",
      "Permite -> 1\n",
      "otimizado -> 1\n",
      "suporte -> 1\n",
      "gráficos -> 1\n",
      "execução -> 1\n",
      "geral -> 1\n",
      "Possui -> 1\n",
      "excelente -> 1\n",
      "conjunto -> 1\n",
      "ferramentas -> 1\n",
      "SQL -> 2\n",
      "estruturados -> 2\n",
      "MLlib -> 1\n",
      "aprendizado -> 1\n",
      "máquina -> 1\n",
      "GraphX -> 1\n",
      "gráfico -> 1\n",
      "Streams -> 1\n",
      "computação -> 1\n",
      "incremental -> 1\n",
      "fluxo -> 1\n"
     ]
    }
   ],
   "source": [
    "# Definir a lista de palavras indesejadas\n",
    "palavras_indesejadas = [\"e\", \"é\", \"um\", \"para\", \"em\", \"como\", \"a\", \"tais\", \"como\", \"de\", \"\"]\n",
    "\n",
    "# Dividir cada linha em palavras e criar um RDD com todas as palavras\n",
    "palavras = rdd_sem_pontuacao.flatMap(lambda linha: linha.split(\" \"))\n",
    "\n",
    "# Filtrar as palavras indesejadas\n",
    "palavras_filtradas = palavras.filter(lambda palavra: palavra not in palavras_indesejadas)\n",
    "\n",
    "# Contar as ocorrências de cada palavra\n",
    "contagem_palavras = palavras_filtradas.countByValue()\n",
    "\n",
    "# Imprimir as palavras únicas e suas contagens\n",
    "print(\"Palavras únicas encontradas e suas contagens:\")\n",
    "for palavra, contagem in contagem_palavras.items():\n",
    "    print(palavra, \"->\", contagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valorPressao(x):\n",
    "    idt = x\n",
    "    pas = random.randint(11, 19)\n",
    "    pad = random.randint(70, 120)\n",
    "    return (x, pas, pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd_pipe = sc.parallelize(range(1000)).map(valorPressao)\n",
    "print(type(rdd_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 17, 92),\n",
       " (1, 18, 102),\n",
       " (2, 19, 100),\n",
       " (3, 18, 78),\n",
       " (4, 12, 94),\n",
       " (5, 11, 89),\n",
       " (6, 12, 82),\n",
       " (7, 14, 79),\n",
       " (8, 18, 100),\n",
       " (9, 15, 76)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_pipe.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = rdd_pipe.toDF((\"Id\", \"PAS\", \"PAD\"))\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| Id|PAS|PAD|\n",
      "+---+---+---+\n",
      "|  0| 19|100|\n",
      "|  1| 14| 89|\n",
      "|  2| 11| 80|\n",
      "|  3| 14|102|\n",
      "|  4| 14|110|\n",
      "|  5| 19| 95|\n",
      "|  6| 16| 78|\n",
      "|  7| 11| 93|\n",
      "|  8| 13| 99|\n",
      "|  9| 13| 91|\n",
      "| 10| 14|103|\n",
      "| 11| 19|117|\n",
      "| 12| 12| 92|\n",
      "| 13| 13| 97|\n",
      "| 14| 11| 74|\n",
      "| 15| 13| 95|\n",
      "| 16| 12| 93|\n",
      "| 17| 16| 85|\n",
      "| 18| 12| 78|\n",
      "| 19| 17| 88|\n",
      "+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizar a seção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
